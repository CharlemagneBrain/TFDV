{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V_OLlgtQ0DW"
   },
   "source": [
    "# Ungraded Lab: TFDV Exercise\n",
    "\n",
    "\n",
    "TFDV fournit des informations sur certaines questions clés du processus d'analyse de données telles que :\n",
    "\n",
    "* Quelles sont les statistiques sous-jacentes de mes données ?\n",
    "\n",
    "* Comment mon jeu de données d'entraînement est-il organisé ?\n",
    "\n",
    "* Comment mon jeu de données d'évaluation et de mise en service se compare-t-il au jeu de données d'entraînement ?\n",
    "\n",
    "* Comment puis-je trouver et corriger les anomalies de données ?\n",
    "\n",
    "La figure ci-dessous résume le processus habituel de TFDV :\n",
    "\n",
    "<img src='img/tfdv.png' alt='picture of tfdv workflow'>\n",
    "\n",
    "En MLOps (ops signifiant \"opérations\"), les termes \"feature skew\", \"schema skew\" et \"distribution skew\" font référence aux différents types de déséquilibres qui peuvent survenir dans les données d'un modèle de machine learning au cours de son cycle de vie.\n",
    "\n",
    "Voici ce que ces termes signifient en détail :\n",
    "\n",
    "**Feature skew** : cela se produit lorsqu'une ou plusieurs colonnes (ou features) d'un jeu de données ont des valeurs manquantes, aberrantes ou non valides. Cela peut affecter négativement la qualité du modèle et sa performance.\n",
    "\n",
    "**Schema skew** : cela se produit lorsqu'il y a des différences dans la structure ou le type des données entre les différentes versions d'un jeu de données. Par exemple, si une colonne a été ajoutée ou supprimée d'une version à l'autre, ou si le type de données d'une colonne a changé. Cela peut entraîner des erreurs lors de l'exécution du modèle.\n",
    "\n",
    "**Distribution skew** : cela se produit lorsqu'il y a des différences dans la distribution des données entre les différentes versions d'un jeu de données. Par exemple, si la proportion de valeurs positives et négatives dans une colonne a changé d'une version à l'autre. Cela peut affecter la qualité du modèle et sa performance.\n",
    "\n",
    "Il est important de surveiller et de corriger ces déséquilibres dans les données pour assurer la qualité et la robustesse du modèle de machine learning.\n",
    "<br/>\n",
    "Comme indiqué, nous pouvons utiliser TFDV pour calculer des statistiques descriptives des données d'entraînement et générer un schéma. Nous pouvons alors valider de nouveaux jeux de données (par exemple, le jeu de données de service de nos clients) contre ce schéma pour détecter et corriger les anomalies. Cela permet d'éviter les différents types de distorsion. De cette façon, nous pouvons être sûrs que notre modèle s'entraîne sur des données ou prédit des données qui sont cohérentes avec les types de caractéristiques et la distribution attendus.\n",
    "- **Générer et visualiser des statistiques à partir d'un jeu de données**\n",
    "- **Détecter et corriger les anomalies dans un jeu de données d'évaluation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyGr4CC8AUfu"
   },
   "source": [
    "## Package Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/charlie/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting tensorflow-data-validation\n",
      "  Downloading tensorflow_data_validation-1.12.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow<7,>=6\n",
      "  Downloading pyarrow-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow-data-validation) (3.19.6)\n",
      "Collecting tensorflow-metadata<1.13,>=1.12.0\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow-data-validation) (1.16.0)\n",
      "Collecting apache-beam[gcp]<3,>=2.40\n",
      "  Downloading apache_beam-2.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow-data-validation) (1.23.4)\n",
      "Collecting tfx-bsl<1.13,>=1.12.0\n",
      "  Downloading tfx_bsl-1.12.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (21.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow-data-validation) (1.3.0)\n",
      "Collecting tensorflow<3,>=2.11\n",
      "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m129.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m99.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas<2,>=1.0\n",
      "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (2.28.1)\n",
      "Collecting zstandard<1,>=0.18.0\n",
      "  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (1.50.0)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.2/526.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle~=2.2.0\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (4.4.0)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (2022.5)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_28_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2020.6.8\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (2.8.2)\n",
      "Collecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Collecting numpy<2,>=1.16\n",
      "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting httplib2<0.21.0,>=0.8\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fasteners<1.0,>=0.3\n",
      "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Collecting objsize<0.6.0,>=0.5.2\n",
      "  Downloading objsize-0.5.2-py3-none-any.whl (8.2 kB)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.1/435.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-httplib2<0.2.0,>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-cloud-spanner<4,>=3.0.0\n",
      "  Downloading google_cloud_spanner-3.26.0-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.3-py2.py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Downloading google_cloud_pubsublite-1.6.0-py2.py3-none-any.whl (271 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.1/271.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-bigquery<4,>=1.6.0\n",
      "  Downloading google_cloud_bigquery-3.4.1-py2.py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.1/215.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (2.13.0)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0\n",
      "  Downloading google_cloud_dlp-3.10.0-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<5,>=3.1.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting google-cloud-bigquery-storage<2.14,>=2.6.3\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-datastore<2,>=1.8.0\n",
      "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
      "  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
      "  Downloading google_cloud_pubsub-2.13.11-py2.py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<3,>=0.28.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (0.27.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (63.4.1)\n",
      "Requirement already satisfied: packaging in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (21.3)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (3.7.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (2.0.7)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (2.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorflow<3,>=2.11->tensorflow-data-validation) (1.6.3)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.0/218.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.11\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.11->tensorflow-data-validation) (0.37.1)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-api-core<3dev,>=1.21.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauth2client>=1.4.12\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (0.2.8)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio-status>=1.16.0\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "Collecting overrides<7.0.0,>=6.0.1\n",
      "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
      "Collecting sqlparse>=0.3.0\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from httplib2<0.21.0,>=0.8->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (0.6.1)\n",
      "Collecting google-auth<3,>=1.18.0\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.0/177.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (1.3.1)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting grpcio!=1.48.0,<2,>=1.33.1\n",
      "  Downloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m667.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-status>=1.16.0\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (5.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.40->tensorflow-data-validation) (0.4.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (3.9.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<3,>=2.11->tensorflow-data-validation) (3.2.2)\n",
      "Building wheels for collected packages: pyfarmhash, crcmod, dill, google-apitools, docopt\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp38-cp38-linux_x86_64.whl size=89070 sha256=e20c193c96990535c94dfe1659eb29b692b95f91e17e04c8fbe7bad718a6fb6a\n",
      "  Stored in directory: /home/charlie/.cache/pip/wheels/ef/32/a2/5f89be144e6e691c3566245279d78d89253e2d3414a842cd35\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-linux_x86_64.whl size=30994 sha256=e3ddcbbce01f23c42eaf851f5a8291ba26c4181ba6140d18dde5ef9eb09413b2\n",
      "  Stored in directory: /home/charlie/.cache/pip/wheels/ca/5a/02/f3acf982a026f3319fb3e798a8dca2d48fafee7761788562e9\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=bf2e8c633746934a4c7f0e96781716117834ac5da125a50b70508536f6c99d4b\n",
      "  Stored in directory: /home/charlie/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131021 sha256=5d330d036a38815810318f8fde020e4a6c92259e655e85f2ad8727127f14298c\n",
      "  Stored in directory: /home/charlie/.cache/pip/wheels/d7/54/79/85de1824f2f4175fb4960c72afb10045d86700c3941dc73685\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e0fb9ad1ff3fec6638a628dbae464b13989acf17d03738c23f17f90ce7b9fd69\n",
      "  Stored in directory: /home/charlie/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built pyfarmhash crcmod dill google-apitools docopt\n",
      "Installing collected packages: pyfarmhash, docopt, crcmod, zstandard, uritemplate, tensorflow-estimator, sqlparse, regex, pymongo, pydot, proto-plus, overrides, orjson, objsize, numpy, keras, joblib, httplib2, googleapis-common-protos, google-crc32c, fasteners, fastavro, dill, cloudpickle, cachetools, tensorflow-metadata, pyarrow, pandas, oauth2client, hdfs, grpcio-status, google-resumable-media, google-auth, grpc-google-iam-v1, google-auth-httplib2, google-apitools, google-api-core, apache-beam, tensorboard, google-cloud-core, google-api-python-client, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow-data-validation\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.13.0\n",
      "    Uninstalling google-auth-2.13.0:\n",
      "      Successfully uninstalled google-auth-2.13.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.10.1\n",
      "    Uninstalling tensorboard-2.10.1:\n",
      "      Successfully uninstalled tensorboard-2.10.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "Successfully installed apache-beam-2.43.0 cachetools-4.2.4 cloudpickle-2.2.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 google-api-core-2.11.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-2.15.0 google-auth-httplib2-0.1.0 google-cloud-bigquery-3.4.1 google-cloud-bigquery-storage-2.13.2 google-cloud-bigtable-1.7.3 google-cloud-core-2.3.2 google-cloud-datastore-1.15.5 google-cloud-dlp-3.10.0 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.11 google-cloud-pubsublite-1.6.0 google-cloud-recommendations-ai-0.7.1 google-cloud-spanner-3.26.0 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 google-crc32c-1.5.0 google-resumable-media-2.4.0 googleapis-common-protos-1.57.0 grpc-google-iam-v1-0.12.4 grpcio-status-1.48.2 hdfs-2.7.0 httplib2-0.20.4 joblib-1.2.0 keras-2.11.0 numpy-1.22.4 oauth2client-4.1.3 objsize-0.5.2 orjson-3.8.3 overrides-6.5.0 pandas-1.5.2 proto-plus-1.22.1 pyarrow-6.0.1 pydot-1.4.2 pyfarmhash-0.3.2 pymongo-3.13.0 regex-2022.10.31 sqlparse-0.4.3 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-data-validation-1.12.0 tensorflow-estimator-2.11.0 tensorflow-metadata-1.12.0 tensorflow-serving-api-2.11.0 tfx-bsl-1.12.0 uritemplate-3.0.1 zstandard-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-data-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GHi-tkOeBOis"
   },
   "outputs": [
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(google-auth 2.13.0 (/home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages), Requirement.parse('google-auth<3.0dev,>=2.14.1'), {'google-api-core'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_data_validation\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfdv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/tensorflow_data_validation/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Init module for TensorFlow Data Validation.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Import stats API.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_data_validation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_sharded_output_suffix\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_data_validation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_sharded_output_supported\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_data_validation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerateStatistics\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/tensorflow_data_validation/api/stats_api.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generator, Text, Optional\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbeam\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_data_validation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m statistics_io_impl\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/apache_beam/__init__.py:93\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpickler\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coders\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m typehints\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/apache_beam/io/__init__.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Protect against environments where clientslibrary is not available.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wrong-import-order, wrong-import-position\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpubsub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gcsio\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/apache_beam/io/gcp/bigquery.py:372\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompressionTypes\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilesystems\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileSystems\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bigquery_schema_tools\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bigquery_tools\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery_io_metadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_bigquery_io_metadata\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/apache_beam/io/gcp/bigquery_schema_tools.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbeam\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery_tools\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypehints\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mapache_beam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto_utils\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/apache_beam/io/gcp/bigquery_tools.py:72\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError, GoogleAPICallError\n\u001b[1;32m     71\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientInfo\n\u001b[0;32m---> 72\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bigquery \u001b[38;5;28;01mas\u001b[39;00m gcp_bigquery\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m   gcp_bigquery \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/google/cloud/bigquery/__init__.py:35\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m bigquery_version\n\u001b[1;32m     33\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m bigquery_version\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AccessEntry\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/google/cloud/bigquery/client.py:69\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientWithProject  \u001b[38;5;66;03m# type: ignore  # pytype: disable=import-error\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery_storage_v1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_query_read\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     70\u001b[0m         DEFAULT_CLIENT_INFO \u001b[38;5;28;01mas\u001b[39;00m DEFAULT_BQSTORAGE_CLIENT_INFO,\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     DEFAULT_BQSTORAGE_CLIENT_INFO \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/google/cloud/bigquery_storage_v1/__init__.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-bigquery-storage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery_storage_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m client\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbigquery_storage_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/pkg_resources/__init__.py:478\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    476\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Requirement\u001b[38;5;241m.\u001b[39mparse(dist)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 478\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string, Requirement, or Distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/pkg_resources/__init__.py:354\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m working_set\u001b[38;5;241m.\u001b[39mfind(moduleOrReq) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmoduleOrReq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/pkg_resources/__init__.py:909\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mrequirements):\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;124;03m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m     needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m needed:\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages/pkg_resources/__init__.py:800\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m req:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# Oops, the \"best\" so far conflicts with a dependency\u001b[39;00m\n\u001b[1;32m    799\u001b[0m     dependent_req \u001b[38;5;241m=\u001b[39m required_by[req]\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VersionConflict(dist, req)\u001b[38;5;241m.\u001b[39mwith_context(dependent_req)\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# push the new requirements onto the stack\u001b[39;00m\n\u001b[1;32m    803\u001b[0m new_requirements \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrequires(req\u001b[38;5;241m.\u001b[39mextras)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (google-auth 2.13.0 (/home/charlie/anaconda3/envs/mlops-week1-lab/lib/python3.8/site-packages), Requirement.parse('google-auth<3.0dev,>=2.14.1'), {'google-api-core'})"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util import add_extra_rows\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "print('TFDV Version: {}'.format(tfdv.__version__))\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kzt9rO9eAGF-"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "You will be working with the [Census Income Dataset](http://archive.ics.uci.edu/ml/datasets/Census+Income), a dataset that can be used to predict if an individual earns more than or less than 50k US Dollars annually. The summary of attribute names with descriptions/expected values is shown below and you can read more about it [in this data description file.](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)\n",
    "\n",
    "\n",
    "* **age**: continuous.\n",
    "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "* **fnlwgt**: continuous.\n",
    "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "* **education-num**: continuous.\n",
    "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "* **sex**: Female, Male.\n",
    "* **capital-gain**: continuous.\n",
    "* **capital-loss**: continuous.\n",
    "* **hours-per-week**: continuous.\n",
    "* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n",
    "Let's load the dataset and split it into training and evaluation sets. We will not shuffle them for consistent results in this demo notebook but you should otherwise in real projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKTfuT2rga-_"
   },
   "outputs": [],
   "source": [
    "# Read in the training and evaluation datasets\n",
    "df = pd.read_csv('data/adult.data', skipinitialspace=True)\n",
    "\n",
    "# Split the dataset. Do not shuffle for this demo notebook.\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RQvgiMRq0pn"
   },
   "source": [
    "Let's see the first few columns of the train and eval sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxDFD6dR0PYH"
   },
   "outputs": [],
   "source": [
    "# Preview the train set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyrmQLCm0a5V"
   },
   "outputs": [],
   "source": [
    "# Preview the eval set\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12_uIoIjr9lv"
   },
   "source": [
    "From these few columns, you can get a first impression of the data. You will notice that most are strings and integers. There are also columns that are mostly zeroes. In the next sections, you will see how to use TFDV to aggregate and process this information so you can inspect it more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra rows\n",
    "\n",
    "To demonstrate how TFDV can detect anomalies later, you will add a few extra rows to the evaluation dataset. These are either malformed or have values that will trigger certain alarms later in this notebook. The code to add these can be seen in the `add_extra_rows()` function of `util.py` found in your Jupyter workspace. You can look at it later and even modify it after you've completed the entire exercise. For now, let's just execute the function and add the rows that we've defined by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra rows\n",
    "eval_df = add_extra_rows(eval_df)\n",
    "\n",
    "# preview the added rows\n",
    "eval_df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duwwrsvf_9bK"
   },
   "source": [
    "## Generate and visualize training dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Nm5E1HAgPU0"
   },
   "source": [
    "You can now compute and visualize the statistics of your training dataset. TFDV accepts three input formats: TensorFlow’s TFRecord, Pandas Dataframe, and CSV file. In this exercise, you will feed in the Pandas Dataframes you generated from the train-test split. \n",
    "\n",
    "You can compute your dataset statistics by using the [`generate_statistics_from_dataframe()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe) method. Under the hood, it distributes the analysis via [Apache Beam](https://beam.apache.org/) which allows it to scale over large datasets.\n",
    "\n",
    "The results returned by this step for numerical and categorical data are summarized in this table:\n",
    "\n",
    "| Numerical Data | Categorical Data   |\n",
    "|:-:|:-:|\n",
    "|Count of data records|Count of data records\n",
    "|% of missing data records|% of missing data records|\n",
    "|Mean, std, min, max|unique records|\n",
    "|% of zero values|Avg string length|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vTx9Qkk4yGc"
   },
   "outputs": [],
   "source": [
    "# Generate training dataset statistics\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86nhDglwuilJ"
   },
   "source": [
    "Once you've generated the statistics, you can easily visualize your results with the [`visualize_statistics()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics) method. This shows a [Facets interface](https://pair-code.github.io/facets/) and is very useful to spot if you have a high amount of missing data or high standard deviation. Run the cell below and explore the different settings in the output interface (e.g. Sort by, Reverse order, Feature search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D1wP3mm5ebW"
   },
   "outputs": [],
   "source": [
    "# Visualize training dataset statistics\n",
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVQTOBpdgPU0"
   },
   "source": [
    "## Infer data schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya6ecHE9gPU1"
   },
   "source": [
    "Next step is to create a data schema to describe your train set. Simply put, a schema describes standard characteristics of your data such as column data types and expected data value range. The schema is created on a dataset that you consider as reference, and can be reused to validate other incoming datasets.\n",
    "\n",
    "With the computed statistics, TFDV allows you to automatically generate an initial version of the schema using the [`infer_schema()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) method. This returns a Schema [protocol buffer](https://developers.google.com/protocol-buffers) containing the result. As mentioned in the [TFX paper](http://stevenwhang.com/tfx_paper.pdf) (Section 3.3), the results of the schema inference can be summarized as follows:\n",
    "\n",
    "* The expected type of each feature.\n",
    "* The expected presence of each feature, in terms of a minimum count and fraction of examples that must contain\n",
    "the feature.\n",
    "* The expected valency of the feature in each example, i.e.,\n",
    "minimum and maximum number of values.\n",
    "* The expected domain of a feature, i.e., the small universe of\n",
    "values for a string feature, or range for an integer feature.\n",
    "\n",
    "Run the cell below to infer the training dataset schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9skjM-M44Jz"
   },
   "outputs": [],
   "source": [
    "# Infer schema from the computed statistics.\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "\n",
    "# Display the inferred schema\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oj_GIprgPU1"
   },
   "source": [
    "## Generate and visualize evaluation dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTYMPukogPU1"
   },
   "source": [
    "The next step after generating the schema is to now look at the evaluation dataset. You will begin by computing its statistics then compare it with the training statistics. It is important that the numerical and categorical features of the evaluation data belongs roughly to the same range as the training data. Otherwise, you might have distribution skew that will negatively affect the accuracy of your model.\n",
    "\n",
    "TFDV allows you to generate both the training and evaluation dataset statistics side-by-side. You can use the [`visualize_statistics()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics) function and pass additional parameters to overlay the statistics from both datasets (referenced as left-hand side and right-hand side statistics). Let's see what these parameters are:\n",
    "\n",
    "- `lhs_statistics`: Required parameter. Expects an instance of `DatasetFeatureStatisticsList `.\n",
    "\n",
    "\n",
    "- `rhs_statistics`: Expects an instance of `DatasetFeatureStatisticsList ` to compare with `lhs_statistics`.\n",
    "\n",
    "\n",
    "- `lhs_name`: Name of the `lhs_statistics` dataset.\n",
    "\n",
    "\n",
    "- `rhs_name`: Name of the `rhs_statistics` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzZy1x3c6Mi0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate evaluation dataset statistics\n",
    "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df)\n",
    "\n",
    "# Compare training with evaluation\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=eval_stats, \n",
    "    rhs_statistics=train_stats, \n",
    "    lhs_name='EVAL_DATASET', \n",
    "    rhs_name='TRAIN_DATASET'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GODDgoHdgPU2"
   },
   "source": [
    "We encourage you to observe the results generated and toggle the menus to practice manipulating the visualization (e.g. sort by missing/zeroes). You'll notice that TFDV detects the malformed rows we introduced earlier. First, the `min` and `max` values of the `age` row shows `0` and `1000`, respectively. We know that those values do not make sense if we're talking about working adults. Secondly, the `workclass` row in the Categorical Features says that `0.02%` of the data is missing that particular attribute. Let's drop these rows to make the data more clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the age range\n",
    "eval_df = eval_df[eval_df['age'] > 16]\n",
    "eval_df = eval_df[eval_df['age'] < 91]\n",
    "\n",
    "# drop missing values\n",
    "eval_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then compute the statistics again and see the difference in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation dataset statistics\n",
    "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df)\n",
    "\n",
    "# Compare training with evaluation\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=eval_stats, \n",
    "    rhs_statistics=train_stats, \n",
    "    lhs_name='EVAL_DATASET', \n",
    "    rhs_name='TRAIN_DATASET'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3Lvnr-YgPU2"
   },
   "source": [
    "## Calculate and display evaluation anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1D9bNcigPU2"
   },
   "source": [
    "You can use your reference schema to check for anomalies such as new values for a specific feature in the evaluation data. Detected anomalies can either be considered a real error that needs to be cleaned, or depending on your domain knowledge and the specific case, they can be accepted. \n",
    "\n",
    "Let's detect and display evaluation anomalies and see if there are any problems that need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OR5dBqpW6ky2"
   },
   "outputs": [],
   "source": [
    "# Check evaluation data for errors by validating the evaluation dataset statistics using the reference schema\n",
    "anomalies =  tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
    "\n",
    "# Visualize anomalies\n",
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeEvhvgqzLu-"
   },
   "source": [
    "## Revising the Schema\n",
    "\n",
    "As shown in the results above, TFDV is able to detect the remaining irregularities we introduced earlier. The short and long descriptions tell us what were detected. As expected, there are string values for `race`, `native-country` and `occupation` that are not found in the domain of the training set schema (you might see a different result if the shuffling of the datasets was applied). What you decide to do about the anomalies depend on your domain knowledge of the data. If an anomaly indicates a data error, then the underlying data should be fixed. Otherwise, you can update the schema to include the values in the evaluation dataset.\n",
    "\n",
    "TFDV provides a set of utility methods and parameters that you can use for revising the inferred schema. This [reference](https://www.tensorflow.org/tfx/data_validation/anomalies) lists down the type of anomalies and the parameters that you can edit but we'll focus only on a couple here.\n",
    "\n",
    "- You can relax the minimum fraction of values that must come from the domain of a particular feature (as described by `ENUM_TYPE_UNEXPECTED_STRING_VALUES` in the [reference](https://www.tensorflow.org/tfx/data_validation/anomalies)):\n",
    "\n",
    "```python\n",
    "tfdv.get_feature(schema, 'feature_column_name').distribution_constraints.min_domain_mass = <float: 0.0 to 1.0>\n",
    "```\n",
    "\n",
    "- You can add a new value to the domain of a particular feature:\n",
    "\n",
    "```python\n",
    "tfdv.get_domain(schema, 'feature_column_name').value.append('string')\n",
    "```\n",
    "\n",
    "Let's use these in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKECg6Lf6-ks"
   },
   "source": [
    "## Fix anomalies in the schema\n",
    "\n",
    "Let's say that we want to accept the string anomalies reported as valid. If you want to tolerate a fraction of missing values from the evaluation dataset, you can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relax the minimum fraction of values that must come from the domain for the feature `native-country`\n",
    "country_feature = tfdv.get_feature(schema, 'native-country')\n",
    "country_feature.distribution_constraints.min_domain_mass = 0.9\n",
    "\n",
    "# Relax the minimum fraction of values that must come from the domain for the feature `occupation`\n",
    "occupation_feature = tfdv.get_feature(schema, 'occupation')\n",
    "occupation_feature.distribution_constraints.min_domain_mass = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be rigid and instead add only valid values to the domain, you can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTCWS04p6lDh"
   },
   "outputs": [],
   "source": [
    "# Add new value to the domain of the feature `race`\n",
    "race_domain = tfdv.get_domain(schema, 'race')\n",
    "race_domain.value.append('Asian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you can also restrict the range of a numerical feature. This will let you know of invalid values without having to inspect it visually (e.g. the invalid `age` values earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the range of the `age` feature\n",
    "tfdv.set_domain(schema, 'age', schema_pb2.IntDomain(name='age', min=17, max=90))\n",
    "\n",
    "# Display the modified schema. Notice the `Domain` column of `age`.\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these revisions, running the validation should now show no anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate eval stats after updating the schema \n",
    "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining dataset slices\n",
    "\n",
    "TFDV also allows you to analyze specific slices of your dataset. This is particularly useful if you want to inspect if a feature type is well-represented in your dataset. Let's walk through an example where we want to compare the statistics for male and female participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will use the [`get_feature_value_slicer`](https://github.com/tensorflow/data-validation/blob/master/tensorflow_data_validation/utils/slicing_util.py#L48) method from the `slicing_util` to get the features you want to examine. You can specify that by passing a dictionary to the `features` argument. If you want to get the entire domain of a feature, then you can map the feature name with `None` as shown below. This means that you will get slices for both `Male` and `Female` entries. This returns a function that can be used to extract the said feature slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_data_validation.utils import slicing_util\n",
    "\n",
    "slice_fn = slicing_util.get_feature_value_slicer(features={'sex': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the slice function ready, you can now generate the statistics. You need to tell TFDV that you need statistics for the features you set and you can do that through the `slice_functions` argument of [`tfdv.StatsOptions`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions). Let's prepare that in the cell below. Notice that you also need to pass in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare stats options\n",
    "slice_stats_options = tfdv.StatsOptions(schema=schema,\n",
    "                                        slice_functions=[slice_fn],\n",
    "                                        infer_type_from_schema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will then pass these options to the `generate_statistics_from_csv()` method. As of writing, generating sliced statistics only works for CSVs so you will need to convert the Pandas dataframe to a CSV. Passing the `slice_stats_options` to `generate_statistics_from_dataframe()` will not produce the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv`\n",
    "CSV_PATH = 'slice_sample.csv'\n",
    "train_df.to_csv(CSV_PATH)\n",
    "\n",
    "# Calculate statistics for the sliced dataset\n",
    "sliced_stats = tfdv.generate_statistics_from_csv(CSV_PATH, stats_options=slice_stats_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, you now have the statistics for the set slice. These are packed into a `DatasetFeatureStatisticsList` protocol buffer. You can see the dataset names below. The first element in the list (i.e. index=0) is named `All_Examples` which just contains the statistics for the entire dataset. The next two elements (i.e. named `sex_Male` and `sex_Female`) are the datasets that contain the stats for the slices. It is important to note that these datasets are of the type: `DatasetFeatureStatistics`. You will see why this is important after the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Datasets generated: {[sliced.name for sliced in sliced_stats.datasets]}')\n",
    "\n",
    "print(f'Type of sliced_stats elements: {type(sliced_stats.datasets[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then visualize the statistics as before to examine the slices. An important caveat is `visualize_statistics()` accepts a `DatasetFeatureStatisticsList` type instead of `DatasetFeatureStatistics`. Thus, at least for this version of TFDV, you will need to convert it to the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList\n",
    "\n",
    "# Convert `Male` statistics (index=1) to the correct type and get the dataset name\n",
    "male_stats_list = DatasetFeatureStatisticsList()\n",
    "male_stats_list.datasets.extend([sliced_stats.datasets[1]])\n",
    "male_stats_name = sliced_stats.datasets[1].name\n",
    "\n",
    "# Convert `Female` statistics (index=2) to the correct type and get the dataset name\n",
    "female_stats_list = DatasetFeatureStatisticsList()\n",
    "female_stats_list.datasets.extend([sliced_stats.datasets[2]])\n",
    "female_stats_name = sliced_stats.datasets[2].name\n",
    "\n",
    "# Visualize the two slices side by side\n",
    "tfdv.visualize_statistics(\n",
    "    lhs_statistics=male_stats_list,\n",
    "    rhs_statistics=female_stats_list,\n",
    "    lhs_name=male_stats_name,\n",
    "    rhs_name=female_stats_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the visualization of the two slices and you can compare how they are represented in the dataset. \n",
    "\n",
    "We encourage you to go back to the beginning of this section and try different slices. Here are other ways you can explore:\n",
    "\n",
    "* If you want to be more specific, then you can map the specific value to the feature name. For example, if you want just `Male`, then you can declare it as `features={'sex': [b'Male']}`. Notice that the string literal needs to be passed in as bytes with the `b'` prefix.\n",
    "\n",
    "* You can also pass in several features if you want. For example, if you want to slice through both the `sex` and `race` features, then you can do `features={'sex': None, 'race': None}`.\n",
    "\n",
    "You might find it cumbersome or inefficient to redo the whole process for a particular slice. For that, you can make helper functions to streamline the type conversions and you will see one implementation in this week's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YY812jDDgPU3"
   },
   "source": [
    "## Wrap up\n",
    "\n",
    "This exercise demonstrated how you would use Tensorflow Data Validation in a machine learning project.\n",
    "\n",
    "* It allows you to scale the computation of statistics over datasets.\n",
    "\n",
    "* You can infer the schema of a given dataset and revise it based on your domain knowledge. \n",
    "\n",
    "* You can inspect discrepancies between the training and evaluation datasets by visualizing the statistics and detecting anomalies.\n",
    "\n",
    "* You can analyze specific slices of your dataset.\n",
    "\n",
    "You can consult this notebook in this week's programming assignment as well as these additional resources:\n",
    "\n",
    "* [TFDV Guide](https://www.tensorflow.org/tfx/data_validation/get_started) \n",
    "* [TFDV blog post](https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html)\n",
    "* [Tensorflow Official Tutorial](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/data_validation/tfdv_basic.ipynb#scrollTo=mPt5BHTwy_0F)\n",
    "* [API Docs](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C2_W1_Lab_1_TFDV_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
